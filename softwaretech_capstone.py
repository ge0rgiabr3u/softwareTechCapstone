# -*- coding: utf-8 -*-
"""SoftwareTech Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L4Vt-9LgD1QeEwnuSuMiuHJm6bfPS02y

## Software Technology Capstone [Assignment 9] - u3242880

DATA SET: Diamond Data available from Kaggle repository
https://www.kaggle.com/code/karnikakapoor/diamond-price-prediction/notebook
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/SoftwareTechCapstone

## !ls /content/drive/MyDrive/SoftwareTechCapstone

"""**1. Reading the Dataset**"""

#supress warning messages
import warnings
warnings.filterwarnings("ignore")

#import libaries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#read dataset
DiamondData=pd.read_csv('diamonds.csv', encoding="latin")

#removing duplicates if any
print("Shape before deleting duplicate values:", DiamondData.shape)
DiamondData=DiamondData.drop_duplicates()
print("Shape after deleting duplicate values", DiamondData.shape)

"""- 53,940 unqiue diamonds
- 10 diamond attributes
- cut, color and clarity are qualitative, the rest are quantitative

---

**2. Problem Statement Definition**: Develop a prediction model which can predict the price of a diamond (in USD) based on the following predictive attributes:
* carat (0.2-5.01) - physical weight of diamond measured in metric carats
* cut (fair, good, very good, premium, ideal)
* color (J (worst) - D (best) - quality of cut
* clarity (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) - how clear the diamond is
* depth = z/mean(x,y)
* table - width of top of diamond relative to widest point
* x dimension - length in mm
* y dimension - width in mm
* z dimension - depth in mm

**3. Target Variable**: diamond price in USD

**4. Target Variable Distribution**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
DiamondData['price'].hist()

"""Positively skewed price distribution. This aligns with the expectation that of more valuable diamonds are increasingly more rare.

For cheaper diamonds there is a large data sample for the machine learning algorithm to learn from however as the amount of diamonds worth more steadily decrease with price, the algorithms predicition of more expensive diamonds may be less reliable. Even so however there seems to be adequate amounts of data in each column.

**5. Basic Data Exploratory Analysis**
"""

print("First five diamonds in the dataset")
DiamondData.head()

print("Last five diamonds in the dataset")
DiamondData.tail()

DiamondData.info()

DiamondData.describe(include='all')
# NaN: not a number

# < 20 unique indicate variable is categorical
DiamondData.nunique()

"""**Observations from Step 5**
*   cut, color and clarity are categorical while the rest are continuous
*   min values of the x, y and z columns is 0 indicating presence of faulty data as the dimensions of a real life diamond cannot be zero
* 'Unamed: 0' column does not affect target variable
* all other columns selected for further analysis

**6. Identifying and Rejecting Unwanted Columns**
"""

# remove Unnamed: 0 column
DiamondData = DiamondData.drop(["Unnamed: 0"], axis =1)
DiamondData.describe(include='all')

"""**7. Visual Exploratory Data Analysis**

**Categorical Predictor Variables in Histograms**
"""

# Commented out IPython magic to ensure Python compatibility.
def PlotBarCharts(inpData, colsToPlot):
#   %matplotlib inline

  import matplotlib.pyplot as plt
  fig, subplot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(20,5))
  fig.suptitle('Bar charts of: '+str(colsToPlot))

  for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):
    inpData.groupby(colName).size().plot(kind='bar', ax=subplot[plotNumber])

PlotBarCharts(inpData=DiamondData, colsToPlot=['cut', 'color', 'clarity'])

"""**Continuous Predictor Variables in Histograms**



"""

DiamondData.hist(['carat','depth','table','x','y','z'], figsize=(18,10))

"""**8. Feature Selection based on Data Distribution**

Selected continuous variables:
* carat
* depth  
* table
* x dimension
* y dimension
* z dimension

**9. Outlier and Missing Value Removal**

No outliers to be treated
"""

# remove faulty data in x, y, z
DiamondData = DiamondData.drop(DiamondData[DiamondData["x"]==0] .index)
DiamondData = DiamondData.drop(DiamondData[DiamondData["y"]==0] .index)
DiamondData = DiamondData.drop(DiamondData[DiamondData["z"]==0] .index)

print("New shape after faulty data removal: " + str(DiamondData.shape))

print("\n"+"Updated Data:")
DiamondData.describe(include='all')

"""Now the min values of x, y, z are consistent with real diamond data."""

# how many missing values in each column
DiamondData.isnull().sum()

"""**10. Visual and Statistic Correlation analysis for selection of best features**

**Continuous vs Continuous [Scatter Chart]**
"""

ContinuousCols=['carat', 'depth', 'table', 'x', 'y', 'z']
for predictor in ContinuousCols:
  DiamondData.plot.scatter(x=predictor, y='price', figsize=(18,10), title=predictor+" vs "+ 'price')

"""Directly proportional (increasing trend) to price: carats and x dimension

y and z also show an increasing trend however less prominent.

Depth and table scatter plots show no clear trend. Therefore elimated.

**Statistical feature selection (Continuous vs Continuous) - Pearson's Correlation Coefficient**
"""

ContinuousCols=['price', 'carat', 'depth', 'table', 'x', 'y', 'z']
CorrelationData=DiamondData[ContinuousCols].corr()
CorrelationData

# if correlation value >0.5: indicate strong relationship
CorrelationData['price'][abs(CorrelationData['price']) > 0.5]

"""FINAL SELECTED continuous:
* carat
* x
* y
* z

**Categorical vs Continuous [Box Plots]**
"""

CategoricalCols=["cut", "color", "clarity"]

fig, PlotCanvas=plt.subplots(nrows=1, ncols=len(CategoricalCols), figsize=(18,5))

for PredictorCol, i in zip (CategoricalCols, range(len(CategoricalCols))):
  DiamondData.boxplot(column="price", by=PredictorCol, figsize=(5,5), vert=True, ax=PlotCanvas[i])

"""Variations in box distribution for each column indicating correlation to target variable.

**Statistical feature selection (Categorical vs Continuous) - ANOVA test**
"""

def Anova(inpData, TargetVariable, CategoricalCols):
  from scipy.stats import f_oneway

  SelectedCategorical=[]

  print("##### ANOVA Results ##### \n")
  for predictor in CategoricalCols:
    CategoricalGroupList = inpData.groupby(predictor)[TargetVariable].apply(list)
    AnovaResults = f_oneway(*CategoricalGroupList)

  if (AnovaResults[1] < 0.05):
    print(predictor,"is correlated with", TargetVariable, '| P-Value:', AnovaResults[1])
    SelectedCategorical.append(predictor)
  else:
    print(predictor, "is NOT correlated with", TargetVariable, '| P-Value:', AnovaResults[1])

  return(SelectedCategorical)

CategoricalCols=['cut', 'color', 'clarity']
Anova(inpData=DiamondData, TargetVariable='price', CategoricalCols=CategoricalCols)

"""According to ANOVA only clarity is correlated with Target Variable
FINAL SELECTED Cateogrical:
* clarity
"""

# final selection of columns to be used in ML model
SelectedColumns = ['carat', 'x', 'y', 'z', 'clarity']
DataForML=DiamondData[SelectedColumns]
DataForML.head()

"""**11. Data Conversion to numeric values for machine learning/predictive analysis**

Of selected columns clarity (ordinal categorical) requires numeric conversion
"""

DataForML['clarity'].replace(["I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"],
                               [1, 2, 3, 4, 5, 6, 7, 8], inplace=True)
DataForML.head()

# Saving final data subset for reference during deployment
DataForML.to_pickle('DataForML.pkl')

"""**12. Training/Testing Sampling and K-fold Cross Validation**

Split 70% of the data for training and 30% for testing.
"""

# Adding target var to data
DataForML['price']=DiamondData['price']
print (DataForML.columns)

TargetVariable ='price'
Predictors = SelectedColumns

X=DataForML[Predictors].values
y=DataForML[TargetVariable].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=428)

"""**13. Investigating Regression Algortithms**

**Linear Regression**
"""

# #Multiple Linear Regression
# from sklearn.linear_model import LinearRegression
# RegModel = LinearRegression()

# # Printing all the parameters of Linear regression
# print(RegModel)

# # Creating the model on Training Data
# LREG=RegModel.fit(X_train,y_train)
# prediction=LREG.predict(X_test)

# from sklearn import metrics
# # Measuring Goodness of fit in Training data
# print('R2 Value:',metrics.r2_score(y_train, LREG.predict(X_train)))

# ###########################################################################
# print('\n##### Model Validation and Accuracy Calculations ##########')

# # Printing some sample values of prediction
# TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
# TestingDataResults[TargetVariable]=y_test
# TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# # Printing sample prediction values
# print(TestingDataResults.head())

# # Calculating the error for each row
# TestingDataResults['APE']=100 * ((abs(
#   TestingDataResults['price']-TestingDataResults['Predictedprice']))/TestingDataResults['price'])

# MAPE=np.mean(TestingDataResults['APE'])
# MedianMAPE=np.median(TestingDataResults['APE'])

# Accuracy =100 - MAPE
# MedianAccuracy=100- MedianMAPE
# print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
# print('Median Accuracy on test data:', MedianAccuracy)

# # Defining a custom function to calculate accuracy
# # Make sure there are no zeros in the Target variable if you are using MAPE
# def Accuracy_Score(orig,pred):
#     MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
#     #print('#'*70,'Accuracy:', 100-MAPE)
#     return(100-MAPE)

# # Custom Scoring MAPE calculation
# from sklearn.metrics import make_scorer
# custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# # Importing cross validation function from sklearn
# from sklearn.model_selection import cross_val_score

# # Running 10-Fold Cross validation on a given algorithm
# # Passing full data X and y because the K-fold will split the data and automatically choose train/test
# Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
# print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
# print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

# """**Decision Tree Regressor**"""

# # Commented out IPython magic to ensure Python compatibility.
# # Decision Trees (Multiple if-else statements!)
# from sklearn.tree import DecisionTreeRegressor
# RegModel = DecisionTreeRegressor(max_depth=5,criterion='friedman_mse')
# # Good Range of Max_depth = 2 to 20

# # Printing all the parameters of Decision Tree
# print(RegModel)

# # Creating the model on Training Data
# DT=RegModel.fit(X_train,y_train)
# prediction=DT.predict(X_test)

# from sklearn import metrics
# # Measuring Goodness of fit in Training data
# print('R2 Value:',metrics.r2_score(y_train, DT.predict(X_train)))

# # Plotting the feature importance for Top 10 most important columns
# # %matplotlib inline
# feature_importances = pd.Series(DT.feature_importances_, index=Predictors)
# feature_importances.nlargest(10).plot(kind='barh')

# ###########################################################################
# print('\n##### Model Validation and Accuracy Calculations ##########')

# # Printing some sample values of prediction
# TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
# TestingDataResults[TargetVariable]=y_test
# TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# # Printing sample prediction values
# print(TestingDataResults.head())

# # Calculating the error for each row
# TestingDataResults['APE']=100 * ((abs(
#   TestingDataResults['price']-TestingDataResults['Predictedprice']))/TestingDataResults['price'])

# MAPE=np.mean(TestingDataResults['APE'])
# MedianMAPE=np.median(TestingDataResults['APE'])

# Accuracy =100 - MAPE
# MedianAccuracy=100- MedianMAPE
# print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
# print('Median Accuracy on test data:', MedianAccuracy)

# # Defining a custom function to calculate accuracy
# # Make sure there are no zeros in the Target variable if you are using MAPE
# def Accuracy_Score(orig,pred):
#     MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
#     #print('#'*70,'Accuracy:', 100-MAPE)
#     return(100-MAPE)

# # Custom Scoring MAPE calculation
# from sklearn.metrics import make_scorer
# custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# # Importing cross validation function from sklearn
# from sklearn.model_selection import cross_val_score

# # Running 10-Fold Cross validation on a given algorithm
# # Passing full data X and y because the K-fold will split the data and automatically choose train/test
# Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
# print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
# print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""**Random Forest Regressor**"""

# # Commented out IPython magic to ensure Python compatibility.
# # Random Forest (Bagging of multiple Decision Trees)
# from sklearn.ensemble import RandomForestRegressor
# RegModel = RandomForestRegressor(max_depth=4, n_estimators=400,criterion='friedman_mse')
# # Good range for max_depth: 2-10 and n_estimators: 100-1000

# # Printing all the parameters of Random Forest
# print(RegModel)

# # Creating the model on Training Data
# RF=RegModel.fit(X_train,y_train)
# prediction=RF.predict(X_test)

# from sklearn import metrics
# # Measuring Goodness of fit in Training data
# print('R2 Value:',metrics.r2_score(y_train, RF.predict(X_train)))

# # Plotting the feature importance for Top 10 most important columns
# # %matplotlib inline
# feature_importances = pd.Series(RF.feature_importances_, index=Predictors)
# feature_importances.nlargest(10).plot(kind='barh')

# ###########################################################################
# print('\n##### Model Validation and Accuracy Calculations ##########')

# # Printing some sample values of prediction
# TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
# TestingDataResults[TargetVariable]=y_test
# TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# # Printing sample prediction values
# print(TestingDataResults.head())

# # Calculating the error for each row
# TestingDataResults['APE']=100 * ((abs(
#   TestingDataResults['price']-TestingDataResults['Predictedprice']))/TestingDataResults['price'])


# MAPE=np.mean(TestingDataResults['APE'])
# MedianMAPE=np.median(TestingDataResults['APE'])

# Accuracy =100 - MAPE
# MedianAccuracy=100- MedianMAPE
# print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
# print('Median Accuracy on test data:', MedianAccuracy)


# # Defining a custom function to calculate accuracy
# # Make sure there are no zeros in the Target variable if you are using MAPE
# def Accuracy_Score(orig,pred):
#     MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
#     #print('#'*70,'Accuracy:', 100-MAPE)
#     return(100-MAPE)

# # Custom Scoring MAPE calculation
# from sklearn.metrics import make_scorer
# custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# # Importing cross validation function from sklearn
# from sklearn.model_selection import cross_val_score

# # Running 10-Fold Cross validation on a given algorithm
# # Passing full data X and y because the K-fold will split the data and automatically choose train/test
# Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
# print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
# print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""**Support Vector Machine (SWM) Regressor**"""

# Commented out IPython magic to ensure Python compatibility.
# Support Vector Machines(SVM)
from sklearn import svm
RegModel = svm.SVR(C=50, kernel='rbf', gamma=0.01)

# Printing all the parameters
print(RegModel)

# Creating the model on Training Data
SVM=RegModel.fit(X_train,y_train)
prediction=SVM.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, SVM.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# The built in attribute SVM.coef_ works only for linear kernel
# %matplotlib inline
#feature_importances = pd.Series(SVM.coef_[0], index=Predictors)
#feature_importances.nlargest(10).plot(kind='barh')

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['price']-TestingDataResults['Predictedprice']))/TestingDataResults['price'])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)

# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""**K-Nearest Neighbor (KNN)**"""

# #kNN
# # K-Nearest Neighbor(KNN)
# from sklearn.neighbors import KNeighborsRegressor
# RegModel = KNeighborsRegressor(n_neighbors=3)

# # Printing all the parameters of KNN
# print(RegModel)

# # Creating the model on Training Data
# KNN=RegModel.fit(X_train,y_train)
# prediction=KNN.predict(X_test)

# from sklearn import metrics
# # Measuring Goodness of fit in Training data
# print('R2 Value:',metrics.r2_score(y_train, KNN.predict(X_train)))

# # Plotting the feature importance for Top 10 most important columns
# # The variable importance chart is not available for KNN

# ###########################################################################
# print('\n##### Model Validation and Accuracy Calculations ##########')

# # Printing some sample values of prediction
# TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
# TestingDataResults[TargetVariable]=y_test
# TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# # Printing sample prediction values
# print(TestingDataResults.head())

# # Calculating the error for each row
# TestingDataResults['APE']=100 * ((abs(
#   TestingDataResults['price']-TestingDataResults['Predictedprice']))/TestingDataResults['price'])

# MAPE=np.mean(TestingDataResults['APE'])
# MedianMAPE=np.median(TestingDataResults['APE'])

# Accuracy =100 - MAPE
# MedianAccuracy=100- MedianMAPE
# print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
# print('Median Accuracy on test data:', MedianAccuracy)

# # Defining a custom function to calculate accuracy
# # Make sure there are no zeros in the Target variable if you are using MAPE
# def Accuracy_Score(orig,pred):
#     MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
#     #print('#'*70,'Accuracy:', 100-MAPE)
#     return(100-MAPE)

# # Custom Scoring MAPE calculation
# from sklearn.metrics import make_scorer
# custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# # Importing cross validation function from sklearn
# from sklearn.model_selection import cross_val_score

# # Running 10-Fold Cross validation on a given algorithm
# # Passing full data X and y because the K-fold will split the data and automatically choose train/test
# Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
# print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
# print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""**14. Selection of Best Model:** Support Vector Machine (SVM) with an average accuracy of 82.84%

**15. Deployment Best Model in Production**
"""

TargetVariable='price'
Predictors=['carat', 'x', 'y', 'z']

X=DataForML[Predictors].values
y=DataForML[TargetVariable].values

# Normalise dataset
from sklearn.preprocessing import Normalizer
PredictorScaler=Normalizer()
PredictorScalerFit=PredictorScaler.fit(X)

X=PredictorScalerFit.transform(X)

# Check for data loss
print(X.shape)
print(y.shape)

from sklearn import svm
RegModel= svm.SVR(C=50, kernel='rbf', gamma=0.01)

# Retrain model with 100% available data
Final_SVM_Model=RegModel.fit(X,y)


import pickle
import os

with open('/content/drive/My Drive/Final_SVM_Model.pk1', 'wb') as fileWriteStream:
  pickle.dump(Final_SVM_Model, fileWriteStream)
  fileWriteStream.close()

print('pickle file of Predictive Model located:'),os.getcwd()

"""**Python Function**"""

from re import IGNORECASE

def FunctionPredictResult(InputData):
  import pandas as pd
  Num_Inputs=InputData.shape[0]

  # Append new data w Training data
  DataForML=pd.read_pickle('DataForML.pkl')
  # Append input data
  InputData = pd.concat([InputData, DataForML], ignore_index=True)

  # In same order as model training
  Predictors=['carat', 'x', 'y', 'z']
  # Generating the input values to the model
  X=InputData[Predictors].values[0:Num_Inputs]

  # Generating the normalised values of X since it was done while model training
  X=PredictorScalerFit.transform(X)

    # Loading the Function from pickle file
  import pickle
  with open('Final_SVM_Model.pk1', 'rb') as fileReadStream:
    PredictionModel=pickle.load(fileReadStream)
    # Don't forget to close the filestream!
    fileReadStream.close()

    # Genrating Predictions
  Prediction=PredictionModel.predict(X)
  PredictionResult=pd.DataFrame(Prediction, columns=['Prediction'])
  return(PredictionResult)

# Create new data to test function with
NewSampleData=pd.DataFrame(data=[[0.5, 6.99, 30.54, 24.3],[3, 8.1, 47.9, 23.2]],columns=['carat', 'x', 'y', 'z'])

print(NewSampleData)

# Calling the Function for prediction
FunctionPredictResult(NewSampleData)

"""API Prediction Function"""

def FunctionGeneratePrediction(inp_carat , inp_x, inp_y, inp_z):

    # Creating a data frame for the model input
    SampleInputData=pd.DataFrame(
     data=[[inp_carat , inp_x, inp_y, inp_z]],
     columns=['carat', 'x', 'y', 'z'])

    # Calling the function defined above using the input parameters
    Predictions=FunctionPredictResult(InputData= SampleInputData)

    # Returning the predictions
    return(Predictions.to_json())

# Function call
FunctionGeneratePrediction( inp_carat=0.8,
                           inp_x=6.5,
                           inp_y=43.3,
                           inp_z=23.2
                             )

from flask import Flask, request, jsonify
import pickle
import pandas as pd
import numpy

app = Flask(__name__)

@app.route('/prediction_api', methods=["GET"])
def prediction_api():
    try:
        # Getting the paramters from API call
        carat_value = float(request.args.get('carat'))
        x_value=float(request.args.get('x'))
        y_value=float(request.args.get('y'))
        z_value=float(request.args.get('z'))

        # Calling the funtion to get predictions
        prediction_from_api=FunctionGeneratePrediction(
                                                       inp_carat=carat_value,
                                                       inp_x=x_value,
                                                       inp_y=y_value,
                                                       inp_z=z_value,
                                                )

        return (prediction_from_api)

    except Exception as e:
        return('Something is not right!:'+str(e))

import os
if __name__ =="__main__":

    # Hosting the API in localhost
    app.run(host='127.0.0.1', port=9000, threaded=True, debug=True, use_reloader=False)
    # Interrupt kernel to stop the API